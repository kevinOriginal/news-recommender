{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/mnt',force_remount=True)\\n\\n# Path에, 구글드라이브 내의 데이터 경로 설정\\npath = '/content/mnt/MyDrive/colab_data/big_data/user_vector.parquet'\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구글 드라이브 사용 시 주석 해제\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/mnt',force_remount=True)\n",
    "\n",
    "# Path에, 구글드라이브 내의 데이터 경로 설정\n",
    "path = '/content/mnt/MyDrive/colab_data/big_data/user_vector.parquet'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  0\n",
      "GeForce RTX 2060 SUPER\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 할당 설정\n",
    "GPU_NUM = 0 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user_vector.parquet\n",
    "path = \"./data/norm_vector.parquet\"\n",
    "df = pd.read_parquet(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 :  torch.Size([33112, 1422])\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cpu')\n",
    "# Convert to torch 데이터를 텐서로 불러옴. cpu에 올리는 이유는 램 부족 때문\n",
    "data = torch.DoubleTensor(df.values).to('cpu')\n",
    "print(\"전체 데이터 : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_news_list_of_user(user_index) :\n",
    "    base_address = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=102&oid={}&aid={}\"\n",
    "    # 참고. 유저가 본 뉴스 목록들\n",
    "    # user_index에 유저 인덱스 값을 넣으면 된다\n",
    "    test_user_vector = torch.DoubleTensor(data[user_index])\n",
    "    print(\"#\"*20)\n",
    "    print(\"Commented News\")\n",
    "    for i in range(0, test_user_vector.shape[0]) :\n",
    "        if test_user_vector[i] > 0 :\n",
    "            ids = df.columns[i]\n",
    "            oid = ids[4:7]\n",
    "            aid = ids[-10:]\n",
    "            #print(oid,aid)\n",
    "            print (base_address.format(oid,aid))\n",
    "    print(\"#\"*20)\n",
    "    print(\"Recommended News\")\n",
    "    #pred = model(test_user_vector.float().to(device))\n",
    "    #pred_val, pred_index = torch.topk(pred,10)\n",
    "    pred_val, pred_index = get_recommend_news(user_index,500,10)\n",
    "    for i in pred_index :\n",
    "        ids = df.columns[i]\n",
    "        oid = ids[4:7]\n",
    "        aid = ids[-10:]\n",
    "        #print(oid,aid)\n",
    "        print (base_address.format(oid,aid))\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_users_vector(main_user_index,num_k) :\n",
    "    main_user = data[main_user_index].to(device)\n",
    "    \n",
    "    # Cosine Similarity \n",
    "    cos = nn.CosineSimilarity(dim= -1, eps=1e-6)\n",
    "\n",
    "    # 높은 Cos값, 인덱스 저장할 리스트\n",
    "    stored_index = []\n",
    "    stored_values = []\n",
    "\n",
    "    # VRAM 부족으로, 10만개씩 끊어서 계산\n",
    "    # 10만개 당 cos_similarity 높은 num_k개씩 뽑아서 저장\n",
    "    for i in range(0, int(data.shape[0]/100000)+1) : \n",
    "        torch.cuda.empty_cache()\n",
    "        val = cos(main_user,data[100000 * i : 100000 *i + 100000].to(device))\n",
    "        values, index = torch.topk(val,num_k)\n",
    "        index = index + 100000*i\n",
    "        stored_index.append(index)\n",
    "        stored_values.append(values)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ### 최종적으로 num_k개 유저 추리기\n",
    "    #\n",
    "    # 텐서로 변환\n",
    "    stored_index = torch.cat(stored_index,0)\n",
    "    stored_values = torch.cat(stored_values,0)\n",
    "    print(stored_index,stored_values)\n",
    "\n",
    "    # 상위 5개 추리기\n",
    "    similar_values, index_of_stored_index = torch.topk(stored_values,num_k)\n",
    "\n",
    "    # 인덱스 변환\n",
    "    similar_index = []\n",
    "    for i in index_of_stored_index :\n",
    "        print(i)\n",
    "        similar_index.append(stored_index[i])\n",
    "    ### 최종적으로 num_k개 유저 추리기 끝\n",
    "    # num_k개 추린 유저들의 벡터를 similar_users에 저장\n",
    "    similar_users = []\n",
    "    for i in range(0,num_k) :\n",
    "      user_id = df.index[similar_index[i]]\n",
    "      similar_value = similar_values[i]\n",
    "      print(\"Index : {:10d}, ID : {:10s}, Similar Value : {}\".format(similar_index[i],user_id,similar_value))\n",
    "      similar_users.append(df.loc[user_id])\n",
    "\n",
    "    similar_users = torch.DoubleTensor(similar_users)\n",
    "    return simiilar_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommend_news(main_user_index, num_k, num_news) :\n",
    "    main_user = data[main_user_index].to(device)\n",
    "    \n",
    "    # Cosine Similarity \n",
    "    cos = nn.CosineSimilarity(dim= -1, eps=1e-6)\n",
    "\n",
    "    # 높은 Cos값, 인덱스 저장할 리스트\n",
    "    stored_index = []\n",
    "    stored_values = []\n",
    "\n",
    "    # VRAM 부족으로, 10만개씩 끊어서 계산\n",
    "    # 10만개 당 cos_similarity 높은 num_k개씩 뽑아서 저장\n",
    "    for i in range(0, int(data.shape[0]/100000)+1) : \n",
    "        torch.cuda.empty_cache()\n",
    "        val = cos(main_user,data[100000 * i : 100000 *i + 100000].to(device))\n",
    "        values, index = torch.topk(val,num_k)\n",
    "        index = index + 100000*i\n",
    "        stored_index.append(index)\n",
    "        stored_values.append(values)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    ### 최종적으로 num_k개 유저 추리기\n",
    "    #\n",
    "    # 텐서로 변환\n",
    "    stored_index = torch.cat(stored_index,0)\n",
    "    stored_values = torch.cat(stored_values,0)\n",
    "\n",
    "    # 상위 5개 추리기\n",
    "    similar_values, index_of_stored_index = torch.topk(stored_values,num_k)\n",
    "\n",
    "    # 인덱스 변환\n",
    "    similar_index = []\n",
    "    for i in index_of_stored_index :\n",
    "\n",
    "        similar_index.append(stored_index[i])\n",
    "    ### 최종적으로 num_k개 유저 추리기 끝\n",
    "    # num_k개 추린 유저들의 벡터를 similar_users에 저장\n",
    "    similar_users = []\n",
    "    for i in range(0,num_k) :\n",
    "      user_id = df.index[similar_index[i]]\n",
    "      similar_value = similar_values[i]\n",
    "      #print(\"Index : {:10d}, ID : {:10s}, Similar Value : {}\".format(similar_index[i],user_id,similar_value))\n",
    "      similar_users.append(df.loc[user_id])\n",
    "\n",
    "    similar_users = torch.DoubleTensor(similar_users)\n",
    "        # num_k명의 벡터 평균치 계산\n",
    "    similar_users_mean = torch.mean(similar_users,dim=0)\n",
    "\n",
    "    # 하나의 평균 벡터가 나옴\n",
    "    similar_users_mean.shape\n",
    "\n",
    "    # num_k명의 평균 벡터와, 주인공 유저와의 벡터 차잇값을 계산\n",
    "    difference_val = similar_users_mean - main_user.cpu()\n",
    "    \n",
    "    # 차이가 가장 많이 나는 값(뉴스) 추출. 차잇값, 인덱스 반환\n",
    "    num_recommend_news = num_news  # 추천할 뉴스의 갯수\n",
    "    recommend_values, recommend_news_index = torch.topk(difference_val,num_recommend_news)\n",
    "    return recommend_values, recommend_news_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 과정들을 통해, 뉴럴넷에서 학습시킬 Y값 텐서를 생성\n",
    "def get_ys(user_index_start,user_index_end) :\n",
    "    ret = []\n",
    "    user_num = user_index_end - user_index_start\n",
    "    for i in range(user_index_start,user_index_end) :\n",
    "        \n",
    "        if i%10 == 0 :\n",
    "            print(\"get_ys : user index :{} / {}\".format(i,user_num))\n",
    "        val, index = get_recommend_news(i,500,10)\n",
    "        y_vec = torch.zeros((data.shape[1]))\n",
    "        for enum,idx in enumerate(index) :\n",
    "            y_vec[idx] = val[enum]\n",
    "        ret.append(y_vec)\n",
    "    ret = torch.reshape(torch.cat(ret),(user_num,1422))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(user_idx) :\n",
    "    val, index = get_recommend_news(user_idx,500,10)\n",
    "    y_vec = torch.zeros((data.shape[1]))\n",
    "    for enum,idx in enumerate(index) :\n",
    "        y_vec[idx] = val[enum]\n",
    "    return y_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# 아래는 위의 과정들을, 뉴럴넷으로 학습시키는거\n",
    "# 1422 차원의 유저 벡터 들어오면,\n",
    "# 1422 차원의 뉴스 벡터 나오는 뉴럴넷\n",
    "\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성(Instantiate)하고, 멤버 변수로\n",
    "        지정합니다.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Variable을 받아서 출력 데이터의 Variable을\n",
    "        반환해야 합니다. Variable 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        모듈을 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "\n",
    "class ThreeLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성(Instantiate)하고, 멤버 변수로\n",
    "        지정합니다.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Variable을 받아서 출력 데이터의 Variable을\n",
    "        반환해야 합니다. Variable 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        모듈을 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h1_relu = self.linear1(x).clamp(min=0)\n",
    "        h2_relu = self.linear2(h1_relu).clamp(min=0)\n",
    "        y_pred = self.linear3(h2_relu)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlinear1 = torch.nn.Linear(1422, 2844, bias=True)\\nlinear2 = torch.nn.Linear(2844, 1800, bias=True)\\nlinear3 = torch.nn.Linear(1800, 1422, bias=True)\\nrelu = torch.nn.ReLU()\\n\\n# Initialization\\ntorch.nn.init.normal_(linear1.weight)\\ntorch.nn.init.normal_(linear2.weight)\\ntorch.nn.init.normal_(linear3.weight)\\n\\n# model\\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\\n# define cost/loss & optimizer\\n#criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\\ncriterion = torch.nn.MSELoss(size_average=False).to(device)\\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\\n\\n#optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "linear1 = torch.nn.Linear(1422, 2844, bias=True)\n",
    "linear2 = torch.nn.Linear(2844, 1800, bias=True)\n",
    "linear3 = torch.nn.Linear(1800, 1422, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# Initialization\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
    "# define cost/loss & optimizer\n",
    "#criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "criterion = torch.nn.MSELoss(size_average=False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Y Values to 10000 Completed\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크롭. 전체 유저 학습하기엔 시간부족..\n",
    "# 0부터 1000까지의 유저에 대해서만 학습\n",
    "\n",
    "# 데이터를 로드할건지, 새로 만들건지\n",
    "# 학습시킬 유저의 인덱스 값\n",
    "USER_NUM = 10000\n",
    "try :\n",
    "    y = torch.load(\"./data_y/norm_y_to_{}.pth\".format(USER_NUM)).to(device)\n",
    "    print(\"Load Y Values to {} Completed\".format(USER_NUM))\n",
    "except :\n",
    "    print(\"Create Y Values to {}\".format(USER_NUM))\n",
    "    y = get_ys(0,USER_NUM)\n",
    "    torch.save(y,\"./data_y/norm_y_to_{}.pth\".format(USER_NUM))\n",
    "    \n",
    "    \n",
    "x = data[0:USER_NUM].float().to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model 11 Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesn-s1\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (linear1): Linear(in_features=1422, out_features=2000, bias=True)\n",
       "  (linear2): Linear(in_features=2000, out_features=1422, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
    "N, D_in, H, D_out = 1, data.shape[1], 2000, data.shape[1]\n",
    "H1, H2 = 3000,2000\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
    "# 감쌉니다.\n",
    "# 앞에서 정의한 클래스를 생성(Instantiating)해서 모델을 구성합니다.\n",
    "#model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 손실함수와 Optimizer를 만듭니다. SGD 생성자에서 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nnLinear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "\n",
    "# 모델 넘버 지정. 데이터에 있으면 로드, 없으면 새로 만듬\n",
    "\n",
    "MODEL_NUM = 11\n",
    "model = None\n",
    "try :\n",
    "    model = torch.load(\"./model/model_{}.pth\".format(MODEL_NUM))\n",
    "    model.eval()\n",
    "    print(\"Load Model {} Completed\".format(MODEL_NUM))\n",
    "except :\n",
    "    if MODEL_NUM == 6 :\n",
    "        model = ThreeLayerNet(D_in, H1, H2, D_out)\n",
    "    \n",
    "    else :\n",
    "        model = TwoLayerNet(D_in, H, D_out)\n",
    "        \n",
    "    print(\"New Model {} Created\".format(MODEL_NUM))\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average=False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1422])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1422])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(start,end) :\n",
    "    results = []\n",
    "    test_len = end - start\n",
    "    for i in range(start,end) :\n",
    "        if (i-start) % (test_len / 100) == 0 :\n",
    "            print(\"testing... \",i)\n",
    "        test1_val, test1_index = get_recommend_news(i,500,10)\n",
    "        test2_val, test2_index = torch.topk(model(data[i].cuda().float()),10)\n",
    "        correct = 0\n",
    "        for j in test1_index.to('cpu') :\n",
    "            if j in test2_index.to('cpu') :\n",
    "                correct +=1\n",
    "        results.append(correct/10)\n",
    "    results = torch.tensor(results)\n",
    "    print(results.mean())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100, loss : 2625.9345703\n",
      "  200, loss : 1831.3181152\n",
      "  300, loss : 1585.1047363\n",
      "  400, loss : 1454.0507812\n",
      "  500, loss : 1339.5275879\n",
      "  600, loss : 1250.5856934\n",
      "  700, loss : 1197.1093750\n",
      "  800, loss : 1162.5134277\n",
      "  900, loss : 1136.2813721\n",
      " 1000, loss : 1113.4467773\n",
      "testing...  19000\n",
      "tensor(0.5400)\n",
      "test\n",
      " 1100, loss : 1091.0869141\n",
      " 1200, loss : 1067.1824951\n",
      " 1300, loss : 1040.6191406\n",
      " 1400, loss : 1010.8964844\n",
      " 1500, loss : 978.2537842\n",
      " 1600, loss : 943.9959717\n",
      " 1700, loss : 910.3666382\n",
      " 1800, loss : 879.9705811\n",
      " 1900, loss : 854.3502197\n",
      " 2000, loss : 833.6513672\n",
      "testing...  19000\n",
      "tensor(0.6300)\n",
      "test\n",
      " 2100, loss : 816.9644775\n",
      " 2200, loss : 803.2198486\n",
      " 2300, loss : 791.4364014\n",
      " 2400, loss : 780.8250732\n",
      " 2500, loss : 770.7817383\n",
      " 2600, loss : 760.8621216\n",
      " 2700, loss : 750.9614258\n",
      " 2800, loss : 741.1746826\n",
      " 2900, loss : 731.7678223\n",
      " 3000, loss : 722.9429932\n",
      "testing...  19000\n",
      "tensor(0.6500)\n",
      "test\n",
      " 3100, loss : 714.8304443\n",
      " 3200, loss : 707.4264526\n",
      " 3300, loss : 700.7252197\n",
      " 3400, loss : 694.6385498\n",
      " 3500, loss : 689.0908203\n",
      " 3600, loss : 684.0639648\n",
      " 3700, loss : 679.5141602\n",
      " 3800, loss : 675.3826294\n",
      " 3900, loss : 671.6375122\n",
      " 4000, loss : 668.2192383\n",
      "testing...  19000\n",
      "tensor(0.6400)\n",
      "test\n",
      " 4100, loss : 665.1010742\n",
      " 4200, loss : 662.2232056\n",
      " 4300, loss : 659.5317383\n",
      " 4400, loss : 657.0096436\n",
      " 4500, loss : 654.6210938\n",
      " 4600, loss : 652.3292236\n",
      " 4700, loss : 650.1154785\n",
      " 4800, loss : 647.9654541\n",
      " 4900, loss : 645.8635254\n",
      " 5000, loss : 643.8035889\n",
      "testing...  19000\n",
      "tensor(0.6500)\n",
      "test\n",
      " 5100, loss : 641.7833252\n",
      " 5200, loss : 639.7829590\n",
      " 5300, loss : 637.8051147\n",
      " 5400, loss : 635.8580322\n",
      " 5500, loss : 633.9461670\n",
      " 5600, loss : 632.0598145\n",
      " 5700, loss : 630.2067871\n",
      " 5800, loss : 628.3721924\n",
      " 5900, loss : 626.5766602\n",
      " 6000, loss : 624.8247070\n",
      "testing...  19000\n",
      "tensor(0.6800)\n",
      "test\n",
      " 6100, loss : 623.1118164\n",
      " 6200, loss : 621.4429932\n",
      " 6300, loss : 619.8171387\n",
      " 6400, loss : 618.2406006\n",
      " 6500, loss : 616.7148438\n",
      " 6600, loss : 615.2343140\n",
      " 6700, loss : 613.7921143\n",
      " 6800, loss : 612.3958740\n",
      " 6900, loss : 611.0417480\n",
      " 7000, loss : 609.7249756\n",
      "testing...  19000\n",
      "tensor(0.6800)\n",
      "test\n",
      " 7100, loss : 608.4418335\n",
      " 7200, loss : 607.1801758\n",
      " 7300, loss : 605.9372559\n",
      " 7400, loss : 604.7130127\n",
      " 7500, loss : 603.5159912\n",
      " 7600, loss : 602.3300781\n",
      " 7700, loss : 601.1436768\n",
      " 7800, loss : 599.9492188\n",
      " 7900, loss : 598.7614136\n",
      " 8000, loss : 597.5841064\n",
      "testing...  19000\n",
      "tensor(0.7000)\n",
      "test\n",
      " 8100, loss : 596.4161987\n",
      " 8200, loss : 595.2504883\n",
      " 8300, loss : 594.1147461\n",
      " 8400, loss : 593.0071411\n",
      " 8500, loss : 591.9279785\n",
      " 8600, loss : 590.8757324\n",
      " 8700, loss : 589.8490601\n",
      " 8800, loss : 588.8457031\n",
      " 8900, loss : 587.8593140\n",
      " 9000, loss : 586.8897705\n",
      "testing...  19000\n",
      "tensor(0.6900)\n",
      "test\n",
      " 9100, loss : 585.9412842\n",
      " 9200, loss : 585.0128174\n",
      " 9300, loss : 584.1029053\n",
      " 9400, loss : 583.2095947\n",
      " 9500, loss : 582.3297119\n",
      " 9600, loss : 581.4628296\n",
      " 9700, loss : 580.6060791\n",
      " 9800, loss : 579.7602539\n",
      " 9900, loss : 578.9269409\n",
      "10000, loss : 578.1030273\n",
      "testing...  19000\n",
      "tensor(0.6800)\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesn-s1\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py:359: UserWarning: Couldn't retrieve source code for container of type TwoLayerNet. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100, loss : 577.2877808\n",
      "10200, loss : 576.4778442\n",
      "10300, loss : 575.6745605\n",
      "10400, loss : 574.8746338\n",
      "10500, loss : 574.0717163\n",
      "10600, loss : 573.2713623\n",
      "10700, loss : 572.4736938\n",
      "10800, loss : 571.6720581\n",
      "10900, loss : 570.8720093\n",
      "11000, loss : 570.0571289\n",
      "testing...  19000\n",
      "tensor(0.6900)\n",
      "test\n",
      "11100, loss : 569.2443848\n",
      "11200, loss : 568.4246826\n",
      "11300, loss : 567.5957031\n",
      "11400, loss : 566.7569580\n",
      "11500, loss : 565.9096680\n",
      "11600, loss : 565.0499268\n",
      "11700, loss : 564.1860352\n",
      "11800, loss : 563.3195801\n",
      "11900, loss : 562.4506836\n",
      "12000, loss : 561.5747070\n",
      "testing...  19000\n",
      "tensor(0.6800)\n",
      "test\n",
      "12100, loss : 560.6913452\n",
      "12200, loss : 559.7928467\n",
      "12300, loss : 558.8767090\n",
      "12400, loss : 557.9470215\n",
      "12500, loss : 557.0067139\n",
      "12600, loss : 556.0491943\n",
      "12700, loss : 555.0766602\n",
      "12800, loss : 554.0848999\n",
      "12900, loss : 553.0791626\n",
      "13000, loss : 552.0594482\n",
      "testing...  19000\n",
      "tensor(0.6900)\n",
      "test\n",
      "13100, loss : 551.0344849\n",
      "13200, loss : 550.0051270\n",
      "13300, loss : 548.9708862\n",
      "13400, loss : 547.9355469\n",
      "13500, loss : 546.8988037\n",
      "13600, loss : 545.8642578\n",
      "13700, loss : 544.8367310\n",
      "13800, loss : 543.8215332\n",
      "13900, loss : 542.8217163\n",
      "14000, loss : 541.8392334\n",
      "testing...  19000\n",
      "tensor(0.7000)\n",
      "test\n",
      "14100, loss : 540.8757324\n",
      "14200, loss : 539.9310303\n",
      "14300, loss : 539.0055542\n",
      "14400, loss : 538.0996704\n",
      "14500, loss : 537.2153931\n",
      "14600, loss : 536.3565674\n",
      "14700, loss : 535.5248413\n",
      "14800, loss : 534.7160645\n",
      "14900, loss : 533.9336548\n",
      "15000, loss : 533.1760254\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "15100, loss : 532.4423828\n",
      "15200, loss : 531.7349854\n",
      "15300, loss : 531.0519409\n",
      "15400, loss : 530.3890991\n",
      "15500, loss : 529.7465820\n",
      "15600, loss : 529.1229858\n",
      "15700, loss : 528.5217896\n",
      "15800, loss : 527.9418945\n",
      "15900, loss : 527.3823853\n",
      "16000, loss : 526.8407593\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "16100, loss : 526.3159180\n",
      "16200, loss : 525.8051758\n",
      "16300, loss : 525.3070068\n",
      "16400, loss : 524.8204956\n",
      "16500, loss : 524.3468018\n",
      "16600, loss : 523.8836670\n",
      "16700, loss : 523.4323730\n",
      "16800, loss : 522.9902344\n",
      "16900, loss : 522.5586548\n",
      "17000, loss : 522.1355591\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "17100, loss : 521.7192993\n",
      "17200, loss : 521.3093872\n",
      "17300, loss : 520.9039917\n",
      "17400, loss : 520.5048828\n",
      "17500, loss : 520.1118164\n",
      "17600, loss : 519.7232056\n",
      "17700, loss : 519.3374023\n",
      "17800, loss : 518.9569092\n",
      "17900, loss : 518.5800781\n",
      "18000, loss : 518.2062378\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "18100, loss : 517.8352661\n",
      "18200, loss : 517.4653320\n",
      "18300, loss : 517.0958252\n",
      "18400, loss : 516.7265625\n",
      "18500, loss : 516.3582764\n",
      "18600, loss : 515.9886475\n",
      "18700, loss : 515.6176758\n",
      "18800, loss : 515.2471924\n",
      "18900, loss : 514.8747559\n",
      "19000, loss : 514.5021973\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "19100, loss : 514.1271973\n",
      "19200, loss : 513.7510986\n",
      "19300, loss : 513.3745117\n",
      "19400, loss : 512.9973145\n",
      "19500, loss : 512.6207275\n",
      "19600, loss : 512.2430420\n",
      "19700, loss : 511.8625488\n",
      "19800, loss : 511.4791870\n",
      "19900, loss : 511.0930481\n",
      "20000, loss : 510.7045898\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "20100, loss : 510.3152466\n",
      "20200, loss : 509.9243774\n",
      "20300, loss : 509.5315552\n",
      "20400, loss : 509.1362305\n",
      "20500, loss : 508.7359619\n",
      "20600, loss : 508.3323059\n",
      "20700, loss : 507.9263916\n",
      "20800, loss : 507.5152893\n",
      "20900, loss : 507.0979614\n",
      "21000, loss : 506.6788940\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "21100, loss : 506.2583008\n",
      "21200, loss : 505.8340454\n",
      "21300, loss : 505.4058838\n",
      "21400, loss : 504.9730225\n",
      "21500, loss : 504.5380249\n",
      "21600, loss : 504.1010132\n",
      "21700, loss : 503.6596680\n",
      "21800, loss : 503.2109375\n",
      "21900, loss : 502.7612305\n",
      "22000, loss : 502.3126221\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "22100, loss : 501.8634644\n",
      "22200, loss : 501.4121704\n",
      "22300, loss : 500.9588013\n",
      "22400, loss : 500.5035095\n",
      "22500, loss : 500.0477295\n",
      "22600, loss : 499.5897522\n",
      "22700, loss : 499.1325684\n",
      "22800, loss : 498.6751709\n",
      "22900, loss : 498.2182617\n",
      "23000, loss : 497.7632446\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "23100, loss : 497.3068237\n",
      "23200, loss : 496.8477478\n",
      "23300, loss : 496.3894043\n",
      "23400, loss : 495.9318848\n",
      "23500, loss : 495.4748535\n",
      "23600, loss : 495.0179749\n",
      "23700, loss : 494.5587769\n",
      "23800, loss : 494.1011963\n",
      "23900, loss : 493.6450195\n",
      "24000, loss : 493.1891785\n",
      "testing...  19000\n",
      "tensor(0.6900)\n",
      "test\n",
      "24100, loss : 492.7313843\n",
      "24200, loss : 492.2741394\n",
      "24300, loss : 491.8171387\n",
      "24400, loss : 491.3588257\n",
      "24500, loss : 490.8985596\n",
      "24600, loss : 490.4357910\n",
      "24700, loss : 489.9727173\n",
      "24800, loss : 489.5111084\n",
      "24900, loss : 489.0483704\n",
      "25000, loss : 488.5848083\n",
      "testing...  19000\n",
      "tensor(0.7000)\n",
      "test\n",
      "25100, loss : 488.1201172\n",
      "25200, loss : 487.6567993\n",
      "25300, loss : 487.1948853\n",
      "25400, loss : 486.7359924\n",
      "25500, loss : 486.2774658\n",
      "25600, loss : 485.8187256\n",
      "25700, loss : 485.3574524\n",
      "25800, loss : 484.8971558\n",
      "25900, loss : 484.4362793\n",
      "26000, loss : 483.9746704\n",
      "testing...  19000\n",
      "tensor(0.7000)\n",
      "test\n",
      "26100, loss : 483.5166016\n",
      "26200, loss : 483.0610352\n",
      "26300, loss : 482.6062012\n",
      "26400, loss : 482.1521606\n",
      "26500, loss : 481.6971130\n",
      "26600, loss : 481.2426147\n",
      "26700, loss : 480.7861328\n",
      "26800, loss : 480.3279419\n",
      "26900, loss : 479.8724060\n",
      "27000, loss : 479.4219971\n",
      "testing...  19000\n",
      "tensor(0.7000)\n",
      "test\n",
      "27100, loss : 478.9728394\n",
      "27200, loss : 478.5218811\n",
      "27300, loss : 478.0702515\n",
      "27400, loss : 477.6198120\n",
      "27500, loss : 477.1712036\n",
      "27600, loss : 476.7242432\n",
      "27700, loss : 476.2774963\n",
      "27800, loss : 475.8326721\n",
      "27900, loss : 475.3894653\n",
      "28000, loss : 474.9459839\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "28100, loss : 474.5045471\n",
      "28200, loss : 474.0638123\n",
      "28300, loss : 473.6229553\n",
      "28400, loss : 473.1821899\n",
      "28500, loss : 472.7416382\n",
      "28600, loss : 472.3022461\n",
      "28700, loss : 471.8642578\n",
      "28800, loss : 471.4295654\n",
      "28900, loss : 470.9949951\n",
      "29000, loss : 470.5596313\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "29100, loss : 470.1268921\n",
      "29200, loss : 469.6966553\n",
      "29300, loss : 469.2712402\n",
      "29400, loss : 468.8487549\n",
      "29500, loss : 468.4244080\n",
      "29600, loss : 467.9994507\n",
      "29700, loss : 467.5745850\n",
      "29800, loss : 467.1501160\n",
      "29900, loss : 466.7259827\n",
      "30000, loss : 466.3035583\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "30100, loss : 465.8834229\n",
      "30200, loss : 465.4663391\n",
      "30300, loss : 465.0519409\n",
      "30400, loss : 464.6430359\n",
      "30500, loss : 464.2369385\n",
      "30600, loss : 463.8320312\n",
      "30700, loss : 463.4287720\n",
      "30800, loss : 463.0278931\n",
      "30900, loss : 462.6281433\n",
      "31000, loss : 462.2298889\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "31100, loss : 461.8371887\n",
      "31200, loss : 461.4481506\n",
      "31300, loss : 461.0627441\n",
      "31400, loss : 460.6770020\n",
      "31500, loss : 460.2946167\n",
      "31600, loss : 459.9150391\n",
      "31700, loss : 459.5388794\n",
      "31800, loss : 459.1647949\n",
      "31900, loss : 458.7938843\n",
      "32000, loss : 458.4255066\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "32100, loss : 458.0595703\n",
      "32200, loss : 457.6972046\n",
      "32300, loss : 457.3391113\n",
      "32400, loss : 456.9832153\n",
      "32500, loss : 456.6306152\n",
      "32600, loss : 456.2807617\n",
      "32700, loss : 455.9349976\n",
      "32800, loss : 455.5906372\n",
      "32900, loss : 455.2494202\n",
      "33000, loss : 454.9118347\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "33100, loss : 454.5755005\n",
      "33200, loss : 454.2415771\n",
      "33300, loss : 453.9085083\n",
      "33400, loss : 453.5785217\n",
      "33500, loss : 453.2487183\n",
      "33600, loss : 452.9223328\n",
      "33700, loss : 452.5982666\n",
      "33800, loss : 452.2743835\n",
      "33900, loss : 451.9500122\n",
      "34000, loss : 451.6255798\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "34100, loss : 451.3044434\n",
      "34200, loss : 450.9837036\n",
      "34300, loss : 450.6634521\n",
      "34400, loss : 450.3458862\n",
      "34500, loss : 450.0269775\n",
      "34600, loss : 449.7070923\n",
      "34700, loss : 449.3909302\n",
      "34800, loss : 449.0780029\n",
      "34900, loss : 448.7665405\n",
      "35000, loss : 448.4587402\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "35100, loss : 448.1526184\n",
      "35200, loss : 447.8487549\n",
      "35300, loss : 447.5476379\n",
      "35400, loss : 447.2496643\n",
      "35500, loss : 446.9544067\n",
      "35600, loss : 446.6618652\n",
      "35700, loss : 446.3716431\n",
      "35800, loss : 446.0839233\n",
      "35900, loss : 445.7948914\n",
      "36000, loss : 445.5062866\n",
      "testing...  19000\n",
      "tensor(0.7100)\n",
      "test\n",
      "36100, loss : 445.2203979\n",
      "36200, loss : 444.9363098\n",
      "36300, loss : 444.6541138\n",
      "36400, loss : 444.3741760\n",
      "36500, loss : 444.0968018\n",
      "36600, loss : 443.8205566\n",
      "36700, loss : 443.5455933\n",
      "36800, loss : 443.2719116\n",
      "36900, loss : 442.9977417\n",
      "37000, loss : 442.7209167\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "37100, loss : 442.4433289\n",
      "37200, loss : 442.1692810\n",
      "37300, loss : 441.8993530\n",
      "37400, loss : 441.6311646\n",
      "37500, loss : 441.3643188\n",
      "37600, loss : 441.0981750\n",
      "37700, loss : 440.8318787\n",
      "37800, loss : 440.5667114\n",
      "37900, loss : 440.3020935\n",
      "38000, loss : 440.0393677\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "38100, loss : 439.7781372\n",
      "38200, loss : 439.5178833\n",
      "38300, loss : 439.2573853\n",
      "38400, loss : 438.9971924\n",
      "38500, loss : 438.7361450\n",
      "38600, loss : 438.4761658\n",
      "38700, loss : 438.2186890\n",
      "38800, loss : 437.9625244\n",
      "38900, loss : 437.7078247\n",
      "39000, loss : 437.4557190\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "39100, loss : 437.2031250\n",
      "39200, loss : 436.9524536\n",
      "39300, loss : 436.7021484\n",
      "39400, loss : 436.4521484\n",
      "39500, loss : 436.2024841\n",
      "39600, loss : 435.9553528\n",
      "39700, loss : 435.7096252\n",
      "39800, loss : 435.4645386\n",
      "39900, loss : 435.2209473\n",
      "40000, loss : 434.9796753\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "40100, loss : 434.7411499\n",
      "40200, loss : 434.5012512\n",
      "40300, loss : 434.2642822\n",
      "40400, loss : 434.0297241\n",
      "40500, loss : 433.7979736\n",
      "40600, loss : 433.5684814\n",
      "40700, loss : 433.3374634\n",
      "40800, loss : 433.1033325\n",
      "40900, loss : 432.8729248\n",
      "41000, loss : 432.6448975\n",
      "testing...  19000\n",
      "tensor(0.7200)\n",
      "test\n",
      "41100, loss : 432.4176025\n",
      "41200, loss : 432.1915894\n",
      "41300, loss : 431.9657593\n",
      "41400, loss : 431.7408142\n",
      "41500, loss : 431.5166931\n",
      "41600, loss : 431.2915039\n",
      "41700, loss : 431.0678101\n",
      "41800, loss : 430.8472290\n",
      "41900, loss : 430.6275024\n",
      "42000, loss : 430.4071045\n",
      "testing...  19000\n",
      "tensor(0.7300)\n",
      "test\n",
      "42100, loss : 430.1877747\n",
      "42200, loss : 429.9683838\n",
      "42300, loss : 429.7487183\n",
      "42400, loss : 429.5290222\n",
      "42500, loss : 429.3088684\n",
      "42600, loss : 429.0892334\n",
      "42700, loss : 428.8695679\n",
      "42800, loss : 428.6515503\n",
      "42900, loss : 428.4355469\n",
      "43000, loss : 428.2200317\n",
      "testing...  19000\n",
      "tensor(0.7300)\n",
      "test\n",
      "43100, loss : 428.0038452\n",
      "43200, loss : 427.7859497\n",
      "43300, loss : 427.5695496\n",
      "43400, loss : 427.3539124\n",
      "43500, loss : 427.1373291\n",
      "43600, loss : 426.9196167\n",
      "43700, loss : 426.7034302\n",
      "43800, loss : 426.4886169\n",
      "43900, loss : 426.2739868\n",
      "44000, loss : 426.0596924\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "44100, loss : 425.8440552\n",
      "44200, loss : 425.6275635\n",
      "44300, loss : 425.4108887\n",
      "44400, loss : 425.1928101\n",
      "44500, loss : 424.9749146\n",
      "44600, loss : 424.7571716\n",
      "44700, loss : 424.5395508\n",
      "44800, loss : 424.3222656\n",
      "44900, loss : 424.1049805\n",
      "45000, loss : 423.8872681\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "45100, loss : 423.6689148\n",
      "45200, loss : 423.4496460\n",
      "45300, loss : 423.2295532\n",
      "45400, loss : 423.0083008\n",
      "45500, loss : 422.7824402\n",
      "45600, loss : 422.5544434\n",
      "45700, loss : 422.3262024\n",
      "45800, loss : 422.0960388\n",
      "45900, loss : 421.8646851\n",
      "46000, loss : 421.6331177\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "46100, loss : 421.4005127\n",
      "46200, loss : 421.1665039\n",
      "46300, loss : 420.9299316\n",
      "46400, loss : 420.6908569\n",
      "46500, loss : 420.4499512\n",
      "46600, loss : 420.2061768\n",
      "46700, loss : 419.9587402\n",
      "46800, loss : 419.7086792\n",
      "46900, loss : 419.4556274\n",
      "47000, loss : 419.2001648\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "47100, loss : 418.9400024\n",
      "47200, loss : 418.6773376\n",
      "47300, loss : 418.4127502\n",
      "47400, loss : 418.1443787\n",
      "47500, loss : 417.8745728\n",
      "47600, loss : 417.6025391\n",
      "47700, loss : 417.3277283\n",
      "47800, loss : 417.0513916\n",
      "47900, loss : 416.7701111\n",
      "48000, loss : 416.4879761\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "48100, loss : 416.2041626\n",
      "48200, loss : 415.9199219\n",
      "48300, loss : 415.6312256\n",
      "48400, loss : 415.3388062\n",
      "48500, loss : 415.0450439\n",
      "48600, loss : 414.7470398\n",
      "48700, loss : 414.4469604\n",
      "48800, loss : 414.1440125\n",
      "48900, loss : 413.8428955\n",
      "49000, loss : 413.5420532\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "49100, loss : 413.2381592\n",
      "49200, loss : 412.9252930\n",
      "49300, loss : 412.6113586\n",
      "49400, loss : 412.2968445\n",
      "49500, loss : 411.9832458\n",
      "49600, loss : 411.6703796\n",
      "49700, loss : 411.3542175\n",
      "49800, loss : 411.0354614\n",
      "49900, loss : 410.7159119\n",
      "50000, loss : 410.3981934\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "50100, loss : 410.0802612\n",
      "50200, loss : 409.7661133\n",
      "50300, loss : 409.4522095\n",
      "50400, loss : 409.1372070\n",
      "50500, loss : 408.8211365\n",
      "50600, loss : 408.5035400\n",
      "50700, loss : 408.1843262\n",
      "50800, loss : 407.8650513\n",
      "50900, loss : 407.5449219\n",
      "51000, loss : 407.2254333\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "51100, loss : 406.9071655\n",
      "51200, loss : 406.5903931\n",
      "51300, loss : 406.2727051\n",
      "51400, loss : 405.9522095\n",
      "51500, loss : 405.6332092\n",
      "51600, loss : 405.3168945\n",
      "51700, loss : 405.0029907\n",
      "51800, loss : 404.6907959\n",
      "51900, loss : 404.3804321\n",
      "52000, loss : 404.0706787\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "52100, loss : 403.7604980\n",
      "52200, loss : 403.4484558\n",
      "52300, loss : 403.1377869\n",
      "52400, loss : 402.8276978\n",
      "52500, loss : 402.5162354\n",
      "52600, loss : 402.2054443\n",
      "52700, loss : 401.8969727\n",
      "52800, loss : 401.5910950\n",
      "52900, loss : 401.2865295\n",
      "53000, loss : 400.9833984\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "53100, loss : 400.6817322\n",
      "53200, loss : 400.3816833\n",
      "53300, loss : 400.0830078\n",
      "53400, loss : 399.7856445\n",
      "53500, loss : 399.4881592\n",
      "53600, loss : 399.1884155\n",
      "53700, loss : 398.8878784\n",
      "53800, loss : 398.5878296\n",
      "53900, loss : 398.2907410\n",
      "54000, loss : 397.9948730\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "54100, loss : 397.6990356\n",
      "54200, loss : 397.4054565\n",
      "54300, loss : 397.1116028\n",
      "54400, loss : 396.8181763\n",
      "54500, loss : 396.5247498\n",
      "54600, loss : 396.2331238\n",
      "54700, loss : 395.9447632\n",
      "54800, loss : 395.6557922\n",
      "54900, loss : 395.3681641\n",
      "55000, loss : 395.0833740\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "55100, loss : 394.8016968\n",
      "55200, loss : 394.5230103\n",
      "55300, loss : 394.2478638\n",
      "55400, loss : 393.9754028\n",
      "55500, loss : 393.7061462\n",
      "55600, loss : 393.4395447\n",
      "55700, loss : 393.1756287\n",
      "55800, loss : 392.9138794\n",
      "55900, loss : 392.6534424\n",
      "56000, loss : 392.3959961\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "56100, loss : 392.1398926\n",
      "56200, loss : 391.8849487\n",
      "56300, loss : 391.6315308\n",
      "56400, loss : 391.3801880\n",
      "56500, loss : 391.1305542\n",
      "56600, loss : 390.8841858\n",
      "56700, loss : 390.6404114\n",
      "56800, loss : 390.3992310\n",
      "56900, loss : 390.1610718\n",
      "57000, loss : 389.9262390\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "57100, loss : 389.6935120\n",
      "57200, loss : 389.4631042\n",
      "57300, loss : 389.2341614\n",
      "57400, loss : 389.0064087\n",
      "57500, loss : 388.7799683\n",
      "57600, loss : 388.5542603\n",
      "57700, loss : 388.3287048\n",
      "57800, loss : 388.1040649\n",
      "57900, loss : 387.8806763\n",
      "58000, loss : 387.6585999\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "58100, loss : 387.4371338\n",
      "58200, loss : 387.2171326\n",
      "58300, loss : 386.9992065\n",
      "58400, loss : 386.7821960\n",
      "58500, loss : 386.5664062\n",
      "58600, loss : 386.3508606\n",
      "58700, loss : 386.1361694\n",
      "58800, loss : 385.9230957\n",
      "58900, loss : 385.7128906\n",
      "59000, loss : 385.5042725\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "59100, loss : 385.2962646\n",
      "59200, loss : 385.0889893\n",
      "59300, loss : 384.8819885\n",
      "59400, loss : 384.6757202\n",
      "59500, loss : 384.4698792\n",
      "59600, loss : 384.2654419\n",
      "59700, loss : 384.0628052\n",
      "59800, loss : 383.8602905\n",
      "59900, loss : 383.6593628\n",
      "60000, loss : 383.4573059\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "60100, loss : 383.2550354\n",
      "60200, loss : 383.0525513\n",
      "60300, loss : 382.8513794\n",
      "60400, loss : 382.6514893\n",
      "60500, loss : 382.4520569\n",
      "60600, loss : 382.2520142\n",
      "60700, loss : 382.0523071\n",
      "60800, loss : 381.8529053\n",
      "60900, loss : 381.6549683\n",
      "61000, loss : 381.4576416\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "61100, loss : 381.2608032\n",
      "61200, loss : 381.0635986\n",
      "61300, loss : 380.8643799\n",
      "61400, loss : 380.6647949\n",
      "61500, loss : 380.4655151\n",
      "61600, loss : 380.2659912\n",
      "61700, loss : 380.0650635\n",
      "61800, loss : 379.8651123\n",
      "61900, loss : 379.6645508\n",
      "62000, loss : 379.4620972\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "62100, loss : 379.2583923\n",
      "62200, loss : 379.0536194\n",
      "62300, loss : 378.8465576\n",
      "62400, loss : 378.6376953\n",
      "62500, loss : 378.4265137\n",
      "62600, loss : 378.2134094\n",
      "62700, loss : 378.0010376\n",
      "62800, loss : 377.7887878\n",
      "62900, loss : 377.5756836\n",
      "63000, loss : 377.3607788\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "63100, loss : 377.1428833\n",
      "63200, loss : 376.9242249\n",
      "63300, loss : 376.7030640\n",
      "63400, loss : 376.4790649\n",
      "63500, loss : 376.2513428\n",
      "63600, loss : 376.0201416\n",
      "63700, loss : 375.7834473\n",
      "63800, loss : 375.5427856\n",
      "63900, loss : 375.2979126\n",
      "64000, loss : 375.0527344\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "64100, loss : 374.8086548\n",
      "64200, loss : 374.5609741\n",
      "64300, loss : 374.3101807\n",
      "64400, loss : 374.0571594\n",
      "64500, loss : 373.8005371\n",
      "64600, loss : 373.5416260\n",
      "64700, loss : 373.2786255\n",
      "64800, loss : 373.0155029\n",
      "64900, loss : 372.7513428\n",
      "65000, loss : 372.4836731\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "65100, loss : 372.2125244\n",
      "65200, loss : 371.9373779\n",
      "65300, loss : 371.6600952\n",
      "65400, loss : 371.3798218\n",
      "65500, loss : 371.1016235\n",
      "65600, loss : 370.8251953\n",
      "65700, loss : 370.5477905\n",
      "65800, loss : 370.2694092\n",
      "65900, loss : 369.9906921\n",
      "66000, loss : 369.7101135\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "66100, loss : 369.4292603\n",
      "66200, loss : 369.1463013\n",
      "66300, loss : 368.8659668\n",
      "66400, loss : 368.5872498\n",
      "66500, loss : 368.3095093\n",
      "66600, loss : 368.0294800\n",
      "66700, loss : 367.7470093\n",
      "66800, loss : 367.4651489\n",
      "66900, loss : 367.1822510\n",
      "67000, loss : 366.8989868\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "67100, loss : 366.6199341\n",
      "67200, loss : 366.3435974\n",
      "67300, loss : 366.0679321\n",
      "67400, loss : 365.7908936\n",
      "67500, loss : 365.5170593\n",
      "67600, loss : 365.2459717\n",
      "67700, loss : 364.9760132\n",
      "67800, loss : 364.7089539\n",
      "67900, loss : 364.4418945\n",
      "68000, loss : 364.1770630\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "68100, loss : 363.9146118\n",
      "68200, loss : 363.6539307\n",
      "68300, loss : 363.3925781\n",
      "68400, loss : 363.1325073\n",
      "68500, loss : 362.8744202\n",
      "68600, loss : 362.6188354\n",
      "68700, loss : 362.3658447\n",
      "68800, loss : 362.1157837\n",
      "68900, loss : 361.8675537\n",
      "69000, loss : 361.6160889\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "69100, loss : 361.3632202\n",
      "69200, loss : 361.1109924\n",
      "69300, loss : 360.8594360\n",
      "69400, loss : 360.6086731\n",
      "69500, loss : 360.3591614\n",
      "69600, loss : 360.1116333\n",
      "69700, loss : 359.8662415\n",
      "69800, loss : 359.6223145\n",
      "69900, loss : 359.3796387\n",
      "70000, loss : 359.1381226\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "70100, loss : 358.8980103\n",
      "70200, loss : 358.6580200\n",
      "70300, loss : 358.4185791\n",
      "70400, loss : 358.1766357\n",
      "70500, loss : 357.9344788\n",
      "70600, loss : 357.6922913\n",
      "70700, loss : 357.4506226\n",
      "70800, loss : 357.2065125\n",
      "70900, loss : 356.9613953\n",
      "71000, loss : 356.7146606\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "71100, loss : 356.4680786\n",
      "71200, loss : 356.2218628\n",
      "71300, loss : 355.9764404\n",
      "71400, loss : 355.7314453\n",
      "71500, loss : 355.4862061\n",
      "71600, loss : 355.2390137\n",
      "71700, loss : 354.9914551\n",
      "71800, loss : 354.7444458\n",
      "71900, loss : 354.4970093\n",
      "72000, loss : 354.2495117\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "72100, loss : 353.9996338\n",
      "72200, loss : 353.7474976\n",
      "72300, loss : 353.4936218\n",
      "72400, loss : 353.2394409\n",
      "72500, loss : 352.9830322\n",
      "72600, loss : 352.7260437\n",
      "72700, loss : 352.4689941\n",
      "72800, loss : 352.2101135\n",
      "72900, loss : 351.9497375\n",
      "73000, loss : 351.6886597\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "73100, loss : 351.4277954\n",
      "73200, loss : 351.1666870\n",
      "73300, loss : 350.9022522\n",
      "73400, loss : 350.6380005\n",
      "73500, loss : 350.3721313\n",
      "73600, loss : 350.1083069\n",
      "73700, loss : 349.8466797\n",
      "73800, loss : 349.5842285\n",
      "73900, loss : 349.3212280\n",
      "74000, loss : 349.0574341\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "74100, loss : 348.7912598\n",
      "74200, loss : 348.5234375\n",
      "74300, loss : 348.2554626\n",
      "74400, loss : 347.9888611\n",
      "74500, loss : 347.7196655\n",
      "74600, loss : 347.4482422\n",
      "74700, loss : 347.1794434\n",
      "74800, loss : 346.9113464\n",
      "74900, loss : 346.6444397\n",
      "75000, loss : 346.3787842\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "75100, loss : 346.1140137\n",
      "75200, loss : 345.8495483\n",
      "75300, loss : 345.5856934\n",
      "75400, loss : 345.3226318\n",
      "75500, loss : 345.0591431\n",
      "75600, loss : 344.7968140\n",
      "75700, loss : 344.5320435\n",
      "75800, loss : 344.2684937\n",
      "75900, loss : 344.0067139\n",
      "76000, loss : 343.7454834\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "76100, loss : 343.4839478\n",
      "76200, loss : 343.2221680\n",
      "76300, loss : 342.9599304\n",
      "76400, loss : 342.6996460\n",
      "76500, loss : 342.4401855\n",
      "76600, loss : 342.1819458\n",
      "76700, loss : 341.9255371\n",
      "76800, loss : 341.6690063\n",
      "76900, loss : 341.4111938\n",
      "77000, loss : 341.1519775\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "77100, loss : 340.8926697\n",
      "77200, loss : 340.6351318\n",
      "77300, loss : 340.3787231\n",
      "77400, loss : 340.1227417\n",
      "77500, loss : 339.8663635\n",
      "77600, loss : 339.6115112\n",
      "77700, loss : 339.3561707\n",
      "77800, loss : 339.1030273\n",
      "77900, loss : 338.8507080\n",
      "78000, loss : 338.5965576\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "78100, loss : 338.3417969\n",
      "78200, loss : 338.0882568\n",
      "78300, loss : 337.8351440\n",
      "78400, loss : 337.5824585\n",
      "78500, loss : 337.3313599\n",
      "78600, loss : 337.0804443\n",
      "78700, loss : 336.8294067\n",
      "78800, loss : 336.5791931\n",
      "78900, loss : 336.3295288\n",
      "79000, loss : 336.0802612\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "79100, loss : 335.8292847\n",
      "79200, loss : 335.5778809\n",
      "79300, loss : 335.3263550\n",
      "79400, loss : 335.0761414\n",
      "79500, loss : 334.8270264\n",
      "79600, loss : 334.5779419\n",
      "79700, loss : 334.3303528\n",
      "79800, loss : 334.0837708\n",
      "79900, loss : 333.8368530\n",
      "80000, loss : 333.5900574\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "80100, loss : 333.3427429\n",
      "80200, loss : 333.0969238\n",
      "80300, loss : 332.8510742\n",
      "80400, loss : 332.6062622\n",
      "80500, loss : 332.3606567\n",
      "80600, loss : 332.1163330\n",
      "80700, loss : 331.8702087\n",
      "80800, loss : 331.6229858\n",
      "80900, loss : 331.3778687\n",
      "81000, loss : 331.1339722\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "81100, loss : 330.8912048\n",
      "81200, loss : 330.6488342\n",
      "81300, loss : 330.4039917\n",
      "81400, loss : 330.1578369\n",
      "81500, loss : 329.9121094\n",
      "81600, loss : 329.6644287\n",
      "81700, loss : 329.4148560\n",
      "81800, loss : 329.1657715\n",
      "81900, loss : 328.9160156\n",
      "82000, loss : 328.6678162\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "82100, loss : 328.4198608\n",
      "82200, loss : 328.1704407\n",
      "82300, loss : 327.9209290\n",
      "82400, loss : 327.6721191\n",
      "82500, loss : 327.4242554\n",
      "82600, loss : 327.1778259\n",
      "82700, loss : 326.9322510\n",
      "82800, loss : 326.6870728\n",
      "82900, loss : 326.4422913\n",
      "83000, loss : 326.1977844\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "83100, loss : 325.9526978\n",
      "83200, loss : 325.7070618\n",
      "83300, loss : 325.4603882\n",
      "83400, loss : 325.2144165\n",
      "83500, loss : 324.9683228\n",
      "83600, loss : 324.7214966\n",
      "83700, loss : 324.4745483\n",
      "83800, loss : 324.2260132\n",
      "83900, loss : 323.9772949\n",
      "84000, loss : 323.7302856\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "84100, loss : 323.4831238\n",
      "84200, loss : 323.2372131\n",
      "84300, loss : 322.9930420\n",
      "84400, loss : 322.7483521\n",
      "84500, loss : 322.5029297\n",
      "84600, loss : 322.2564087\n",
      "84700, loss : 322.0075989\n",
      "84800, loss : 321.7584839\n",
      "84900, loss : 321.5101318\n",
      "85000, loss : 321.2624817\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "85100, loss : 321.0149536\n",
      "85200, loss : 320.7671204\n",
      "85300, loss : 320.5193481\n",
      "85400, loss : 320.2728271\n",
      "85500, loss : 320.0272217\n",
      "85600, loss : 319.7809753\n",
      "85700, loss : 319.5360413\n",
      "85800, loss : 319.2922363\n",
      "85900, loss : 319.0488892\n",
      "86000, loss : 318.8047180\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "86100, loss : 318.5623779\n",
      "86200, loss : 318.3190002\n",
      "86300, loss : 318.0754395\n",
      "86400, loss : 317.8341370\n",
      "86500, loss : 317.5946655\n",
      "86600, loss : 317.3549194\n",
      "86700, loss : 317.1163940\n",
      "86800, loss : 316.8785706\n",
      "86900, loss : 316.6414185\n",
      "87000, loss : 316.4058838\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "87100, loss : 316.1720886\n",
      "87200, loss : 315.9393005\n",
      "87300, loss : 315.7084351\n",
      "87400, loss : 315.4785156\n",
      "87500, loss : 315.2497559\n",
      "87600, loss : 315.0231934\n",
      "87700, loss : 314.7971802\n",
      "87800, loss : 314.5718994\n",
      "87900, loss : 314.3471680\n",
      "88000, loss : 314.1228638\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "88100, loss : 313.9010010\n",
      "88200, loss : 313.6802368\n",
      "88300, loss : 313.4595337\n",
      "88400, loss : 313.2381592\n",
      "88500, loss : 313.0170898\n",
      "88600, loss : 312.7976074\n",
      "88700, loss : 312.5788879\n",
      "88800, loss : 312.3612671\n",
      "88900, loss : 312.1439514\n",
      "89000, loss : 311.9284668\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "89100, loss : 311.7130737\n",
      "89200, loss : 311.4985046\n",
      "89300, loss : 311.2832642\n",
      "89400, loss : 311.0687866\n",
      "89500, loss : 310.8554382\n",
      "89600, loss : 310.6427002\n",
      "89700, loss : 310.4307251\n",
      "89800, loss : 310.2194214\n",
      "89900, loss : 310.0076294\n",
      "90000, loss : 309.7966309\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "90100, loss : 309.5861206\n",
      "90200, loss : 309.3760376\n",
      "90300, loss : 309.1655273\n",
      "90400, loss : 308.9546509\n",
      "90500, loss : 308.7442322\n",
      "90600, loss : 308.5342407\n",
      "90700, loss : 308.3255615\n",
      "90800, loss : 308.1179810\n",
      "90900, loss : 307.9113770\n",
      "91000, loss : 307.7052307\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "91100, loss : 307.4994507\n",
      "91200, loss : 307.2940674\n",
      "91300, loss : 307.0892944\n",
      "91400, loss : 306.8857117\n",
      "91500, loss : 306.6831360\n",
      "91600, loss : 306.4812012\n",
      "91700, loss : 306.2803345\n",
      "91800, loss : 306.0803833\n",
      "91900, loss : 305.8804016\n",
      "92000, loss : 305.6802979\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "92100, loss : 305.4793091\n",
      "92200, loss : 305.2798157\n",
      "92300, loss : 305.0806885\n",
      "92400, loss : 304.8821411\n",
      "92500, loss : 304.6845093\n",
      "92600, loss : 304.4881592\n",
      "92700, loss : 304.2929077\n",
      "92800, loss : 304.0990295\n",
      "92900, loss : 303.9058533\n",
      "93000, loss : 303.7133484\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "93100, loss : 303.5209961\n",
      "93200, loss : 303.3281860\n",
      "93300, loss : 303.1359863\n",
      "93400, loss : 302.9448547\n",
      "93500, loss : 302.7541504\n",
      "93600, loss : 302.5640259\n",
      "93700, loss : 302.3741455\n",
      "93800, loss : 302.1855469\n",
      "93900, loss : 301.9974060\n",
      "94000, loss : 301.8105469\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "94100, loss : 301.6242981\n",
      "94200, loss : 301.4378967\n",
      "94300, loss : 301.2521667\n",
      "94400, loss : 301.0670166\n",
      "94500, loss : 300.8814697\n",
      "94600, loss : 300.6968384\n",
      "94700, loss : 300.5132446\n",
      "94800, loss : 300.3303223\n",
      "94900, loss : 300.1482239\n",
      "95000, loss : 299.9669800\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "95100, loss : 299.7853699\n",
      "95200, loss : 299.6029968\n",
      "95300, loss : 299.4212036\n",
      "95400, loss : 299.2396851\n",
      "95500, loss : 299.0591431\n",
      "95600, loss : 298.8798828\n",
      "95700, loss : 298.7005005\n",
      "95800, loss : 298.5208740\n",
      "95900, loss : 298.3409119\n",
      "96000, loss : 298.1617432\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "96100, loss : 297.9840088\n",
      "96200, loss : 297.8070679\n",
      "96300, loss : 297.6311646\n",
      "96400, loss : 297.4570923\n",
      "96500, loss : 297.2835999\n",
      "96600, loss : 297.1105347\n",
      "96700, loss : 296.9379883\n",
      "96800, loss : 296.7659912\n",
      "96900, loss : 296.5949707\n",
      "97000, loss : 296.4243164\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "97100, loss : 296.2536316\n",
      "97200, loss : 296.0836792\n",
      "97300, loss : 295.9142456\n",
      "97400, loss : 295.7454224\n",
      "97500, loss : 295.5777283\n",
      "97600, loss : 295.4104004\n",
      "97700, loss : 295.2443237\n",
      "97800, loss : 295.0783081\n",
      "97900, loss : 294.9120178\n",
      "98000, loss : 294.7451782\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "98100, loss : 294.5790405\n",
      "98200, loss : 294.4135132\n",
      "98300, loss : 294.2487183\n",
      "98400, loss : 294.0844421\n",
      "98500, loss : 293.9206848\n",
      "98600, loss : 293.7581787\n",
      "98700, loss : 293.5967407\n",
      "98800, loss : 293.4349976\n",
      "98900, loss : 293.2733154\n",
      "99000, loss : 293.1122131\n",
      "testing...  19000\n",
      "tensor(0.7700)\n",
      "test\n",
      "99100, loss : 292.9514771\n",
      "99200, loss : 292.7907104\n",
      "99300, loss : 292.6296997\n",
      "99400, loss : 292.4686584\n",
      "99500, loss : 292.3076782\n",
      "99600, loss : 292.1477356\n",
      "99700, loss : 291.9876709\n",
      "99800, loss : 291.8280029\n",
      "99900, loss : 291.6692810\n",
      "100000, loss : 291.5114746\n",
      "testing...  19000\n",
      "tensor(0.7700)\n",
      "test\n",
      "100100, loss : 291.3546753\n",
      "100200, loss : 291.1988525\n",
      "100300, loss : 291.0430298\n",
      "100400, loss : 290.8878784\n",
      "100500, loss : 290.7332764\n",
      "100600, loss : 290.5796509\n",
      "100700, loss : 290.4261475\n",
      "100800, loss : 290.2728882\n",
      "100900, loss : 290.1204834\n",
      "101000, loss : 289.9690857\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "101100, loss : 289.8179321\n",
      "101200, loss : 289.6655884\n",
      "101300, loss : 289.5134888\n",
      "101400, loss : 289.3624573\n",
      "101500, loss : 289.2120972\n",
      "101600, loss : 289.0617065\n",
      "101700, loss : 288.9119873\n",
      "101800, loss : 288.7623901\n",
      "101900, loss : 288.6128540\n",
      "102000, loss : 288.4638672\n",
      "testing...  19000\n",
      "tensor(0.7600)\n",
      "test\n",
      "102100, loss : 288.3155518\n",
      "102200, loss : 288.1678162\n",
      "102300, loss : 288.0203247\n",
      "102400, loss : 287.8733521\n",
      "102500, loss : 287.7265625\n",
      "102600, loss : 287.5804138\n",
      "102700, loss : 287.4340210\n",
      "102800, loss : 287.2873535\n",
      "102900, loss : 287.1412964\n",
      "103000, loss : 286.9950562\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "103100, loss : 286.8483887\n",
      "103200, loss : 286.7014160\n",
      "103300, loss : 286.5547180\n",
      "103400, loss : 286.4088745\n",
      "103500, loss : 286.2625732\n",
      "103600, loss : 286.1170654\n",
      "103700, loss : 285.9716797\n",
      "103800, loss : 285.8268433\n",
      "103900, loss : 285.6820984\n",
      "104000, loss : 285.5370483\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "104100, loss : 285.3913269\n",
      "104200, loss : 285.2470093\n",
      "104300, loss : 285.1030273\n",
      "104400, loss : 284.9599609\n",
      "104500, loss : 284.8171997\n",
      "104600, loss : 284.6742554\n",
      "104700, loss : 284.5318604\n",
      "104800, loss : 284.3896484\n",
      "104900, loss : 284.2468567\n",
      "105000, loss : 284.1046753\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "105100, loss : 283.9626160\n",
      "105200, loss : 283.8201904\n",
      "105300, loss : 283.6781921\n",
      "105400, loss : 283.5353394\n",
      "105500, loss : 283.3923950\n",
      "105600, loss : 283.2498169\n",
      "105700, loss : 283.1075439\n",
      "105800, loss : 282.9653625\n",
      "105900, loss : 282.8215332\n",
      "106000, loss : 282.6779175\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "106100, loss : 282.5352478\n",
      "106200, loss : 282.3927612\n",
      "106300, loss : 282.2502441\n",
      "106400, loss : 282.1081238\n",
      "106500, loss : 281.9666138\n",
      "106600, loss : 281.8248291\n",
      "106700, loss : 281.6831055\n",
      "106800, loss : 281.5427856\n",
      "106900, loss : 281.4031677\n",
      "107000, loss : 281.2632751\n",
      "testing...  19000\n",
      "tensor(0.7400)\n",
      "test\n",
      "107100, loss : 281.1233521\n",
      "107200, loss : 280.9835815\n",
      "107300, loss : 280.8439331\n",
      "107400, loss : 280.7044678\n",
      "107500, loss : 280.5648193\n",
      "107600, loss : 280.4255371\n",
      "107700, loss : 280.2863159\n",
      "107800, loss : 280.1469421\n",
      "107900, loss : 280.0082092\n",
      "108000, loss : 279.8701782\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "108100, loss : 279.7330933\n",
      "108200, loss : 279.5963135\n",
      "108300, loss : 279.4605103\n",
      "108400, loss : 279.3249512\n",
      "108500, loss : 279.1901855\n",
      "108600, loss : 279.0558167\n",
      "108700, loss : 278.9217529\n",
      "108800, loss : 278.7869568\n",
      "108900, loss : 278.6525574\n",
      "109000, loss : 278.5178833\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "109100, loss : 278.3838501\n",
      "109200, loss : 278.2505493\n",
      "109300, loss : 278.1182861\n",
      "109400, loss : 277.9867554\n",
      "109500, loss : 277.8559265\n",
      "109600, loss : 277.7245483\n",
      "109700, loss : 277.5932617\n",
      "109800, loss : 277.4621582\n",
      "109900, loss : 277.3303833\n",
      "110000, loss : 277.1990356\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "110100, loss : 277.0677490\n",
      "110200, loss : 276.9367676\n",
      "110300, loss : 276.8066711\n",
      "110400, loss : 276.6773987\n",
      "110500, loss : 276.5481873\n",
      "110600, loss : 276.4195862\n",
      "110700, loss : 276.2911377\n",
      "110800, loss : 276.1624756\n",
      "110900, loss : 276.0338440\n",
      "111000, loss : 275.9058838\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "111100, loss : 275.7776489\n",
      "111200, loss : 275.6494751\n",
      "111300, loss : 275.5215149\n",
      "111400, loss : 275.3939819\n",
      "111500, loss : 275.2664185\n",
      "111600, loss : 275.1398010\n",
      "111700, loss : 275.0137939\n",
      "111800, loss : 274.8878784\n",
      "111900, loss : 274.7627563\n",
      "112000, loss : 274.6376343\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "112100, loss : 274.5122375\n",
      "112200, loss : 274.3870850\n",
      "112300, loss : 274.2626038\n",
      "112400, loss : 274.1384277\n",
      "112500, loss : 274.0140991\n",
      "112600, loss : 273.8902588\n",
      "112700, loss : 273.7671509\n",
      "112800, loss : 273.6444092\n",
      "112900, loss : 273.5218811\n",
      "113000, loss : 273.3992920\n",
      "testing...  19000\n",
      "tensor(0.7500)\n",
      "test\n",
      "113100, loss : 273.2770081\n",
      "113200, loss : 273.1546021\n",
      "113300, loss : 273.0318604\n",
      "113400, loss : 272.9083252\n",
      "113500, loss : 272.7852783\n",
      "113600, loss : 272.6622314\n",
      "113700, loss : 272.5389404\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "#print(\"DATA TO : {}, MODEL NUM : {}\".format(USER_NUM,MODEL_NUM))\n",
    "#y = torch.where(x==0, y , torch.ones(y.shape).to(device) )\n",
    "\n",
    "while (True) :\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다.\n",
    "    #print(x.dtype)\n",
    "    iteration += 1\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    #print(y_pred.dtype,y.dtype)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iteration % 100 == 0:\n",
    "        print(\"{:5d}, loss : {:.7f}\".format(iteration, loss.item()))\n",
    "    \n",
    "    if iteration % 1000 == 0 :\n",
    "        test(19000,19010)\n",
    "        print(\"test\")\n",
    "    if iteration % 10000 == 0 :\n",
    "        torch.save(model,\"./model/model_{}.pth\".format(MODEL_NUM))\n",
    "        \n",
    "    if loss < 0.1 : \n",
    "        print(\"Loss : {}, Learning exit\".format(loss.item()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "torch.save(model,\"./model/model_{}.pth\".format(MODEL_NUM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_val, test1_index = get_recommend_news(20002,500,10)\n",
    "test2_val, test2_index = torch.topk(model(data[20002].cuda().float()),10)\n",
    "print(\"기존 알고리즘이 추천하는 뉴스 인덱스 : \\n{}, \\n 추천값:{}\".format(test1_index, test1_val))\n",
    "print(\"뉴럴넷이 추천하는 뉴스 인덱스 : \\n{},\\n 추천값:{}\".format(test2_index, test2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news_list_of_user(25016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(start,end) :\n",
    "    results = []\n",
    "    for i in range(start,end) :\n",
    "        test1_val, test1_index = get_recommend_news(i,500,10)\n",
    "        test2_val, test2_index = torch.topk(model(data[i].cuda().float()),10)\n",
    "        correct = 0\n",
    "        for j in test1_index.to('cpu') :\n",
    "            if j in test2_index.to('cpu') :\n",
    "                correct +=1\n",
    "        results.append(correct/10)\n",
    "    results = torch.tensor(results)\n",
    "    print(results.mean())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(29000,29050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.tensor(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
